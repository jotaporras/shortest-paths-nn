{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e23 SparseGT Tuning Analysis\n",
    "\n",
    "This notebook analyzes the hyperparameter tuning experiment for SparseGT on res04.\n",
    "\n",
    "**Grid Search Parameters:**\n",
    "- `num_hops`: [3, 5, 7]\n",
    "- `num_layers`: [2, 3, 4, 5]\n",
    "- `hidden_dim`: [64, 128, 256]\n",
    "- `dropout` (and `attn_dropout`): [0.0, 0.01, 0.05]\n",
    "\n",
    "**Fixed Settings:**\n",
    "- Resolution: res04\n",
    "- Epochs: 10\n",
    "- lr: 0.0001\n",
    "- batch_size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import wandb_buddy as wb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = \"e23_sgt_tuning\"\n",
    "ENTITY = \"alelab\"\n",
    "PROJECT = \"terrains\"\n",
    "\n",
    "# Metrics\n",
    "ERROR_COL = 'val_mae'\n",
    "ERROR_LABEL = 'Validation MAE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Runs from Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fetching runs with tag: {TAG}\")\n",
    "df = wb.load_runs(ENTITY, PROJECT, tags=[TAG])\n",
    "print(f\"Found {len(df)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run states\n",
    "print(\"Run states:\")\n",
    "print(df['state'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Hyperparameters\n",
    "\n",
    "The SparseGT config is logged to wandb with columns like `sparse_gt_num_hops`, `sparse_gt_num_layers`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available sparse_gt columns\n",
    "sgt_cols = [c for c in df.columns if 'sparse_gt' in c.lower()]\n",
    "print(\"SparseGT config columns:\")\n",
    "for col in sgt_cols:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for easier access\n",
    "hp_mapping = {\n",
    "    'sparse_gt_num_hops': 'num_hops',\n",
    "    'sparse_gt_num_layers': 'num_layers', \n",
    "    'sparse_gt_hidden_dim': 'hidden_dim',\n",
    "    'sparse_gt_dropout': 'dropout',\n",
    "    'sparse_gt_attn_dropout': 'attn_dropout'\n",
    "}\n",
    "\n",
    "for old_col, new_col in hp_mapping.items():\n",
    "    if old_col in df.columns:\n",
    "        df[new_col] = df[old_col]\n",
    "\n",
    "# Verify hyperparameters are extracted\n",
    "print(\"Hyperparameter coverage:\")\n",
    "for col in ['num_hops', 'num_layers', 'hidden_dim', 'dropout']:\n",
    "    if col in df.columns:\n",
    "        print(f\"  {col}: {df[col].notna().sum()}/{len(df)} valid, values: {sorted(df[col].dropna().unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Validation MAE from History\n",
    "\n",
    "Similar to the e23 analysis, we fetch `val_batch_mae` history from wandb and average over all logged batches to get the true validation MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_history_mean(entity: str, project: str, run_id: str, metric: str) -> float:\n",
    "    \"\"\"\n",
    "    Fetch the full history of a metric from a wandb run and return its mean.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "    history = run.history(keys=[metric], pandas=True)\n",
    "    \n",
    "    if history.empty or metric not in history.columns:\n",
    "        return np.nan\n",
    "    \n",
    "    values = history[metric].dropna()\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return values.mean()\n",
    "\n",
    "\n",
    "def add_metric_from_history(df: pd.DataFrame, entity: str, project: str, \n",
    "                            source_metric: str, target_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column to df by fetching metric history from wandb and computing the mean.\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    for run_id in tqdm(df['id'], desc=f\"Fetching {source_metric} history\"):\n",
    "        mean_val = get_metric_history_mean(entity, project, run_id, source_metric)\n",
    "        means.append(mean_val)\n",
    "    \n",
    "    df[target_col] = means\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch val_mae from history\n",
    "# First check if val_mae is already available in summary\n",
    "if 'val_mae' in df.columns and df['val_mae'].notna().sum() > 0:\n",
    "    print(f\"Using existing val_mae from summary: {df['val_mae'].notna().sum()}/{len(df)} valid\")\n",
    "else:\n",
    "    print(\"Fetching val_mae from val_batch_mae history...\")\n",
    "    df = add_metric_from_history(df, ENTITY, PROJECT, 'val_batch_mae', 'val_mae')\n",
    "    print(f\"Runs with valid val_mae: {df['val_mae'].notna().sum()}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to valid runs with metrics\n",
    "df_valid = df[df[ERROR_COL].notna()].copy()\n",
    "print(f\"Valid runs for analysis: {len(df_valid)}\")\n",
    "\n",
    "# Display summary sorted by val_mae\n",
    "summary_cols = ['name', 'state', 'num_hops', 'num_layers', 'hidden_dim', 'dropout', ERROR_COL]\n",
    "available_cols = [c for c in summary_cols if c in df_valid.columns]\n",
    "df_valid[available_cols].sort_values(ERROR_COL).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_valid) > 0:\n",
    "    best_idx = df_valid[ERROR_COL].idxmin()\n",
    "    best_run = df_valid.loc[best_idx]\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"BEST CONFIGURATION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  Run: {best_run['name']}\")\n",
    "    print(f\"  {ERROR_LABEL}: {best_run[ERROR_COL]:.6f}\")\n",
    "    print()\n",
    "    print(\"Hyperparameters:\")\n",
    "    for col in ['num_hops', 'num_layers', 'hidden_dim', 'dropout']:\n",
    "        if col in best_run.index:\n",
    "            print(f\"  {col}: {best_run[col]}\")\n",
    "else:\n",
    "    print(\"No valid runs found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Analysis Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for each hyperparameter\n",
    "if len(df_valid) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    hp_cols = ['num_hops', 'num_layers', 'hidden_dim', 'dropout']\n",
    "    \n",
    "    for ax, col in zip(axes.flat, hp_cols):\n",
    "        if col in df_valid.columns:\n",
    "            sns.boxplot(data=df_valid, x=col, y=ERROR_COL, ax=ax)\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(ERROR_LABEL)\n",
    "            ax.set_title(f'{ERROR_LABEL} by {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid runs to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: num_hops vs num_layers (averaged over other params)\n",
    "if len(df_valid) > 0 and 'num_hops' in df_valid.columns and 'num_layers' in df_valid.columns:\n",
    "    pivot_hops_layers = df_valid.pivot_table(\n",
    "        values=ERROR_COL, \n",
    "        index='num_layers', \n",
    "        columns='num_hops', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(pivot_hops_layers, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax)\n",
    "    ax.set_title(f'{ERROR_LABEL}: num_layers vs num_hops')\n",
    "    ax.set_xlabel('num_hops')\n",
    "    ax.set_ylabel('num_layers')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: hidden_dim vs dropout (averaged over other params)\n",
    "if len(df_valid) > 0 and 'hidden_dim' in df_valid.columns and 'dropout' in df_valid.columns:\n",
    "    pivot_dim_drop = df_valid.pivot_table(\n",
    "        values=ERROR_COL, \n",
    "        index='dropout', \n",
    "        columns='hidden_dim', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(pivot_dim_drop, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax)\n",
    "    ax.set_title(f'{ERROR_LABEL}: dropout vs hidden_dim')\n",
    "    ax.set_xlabel('hidden_dim')\n",
    "    ax.set_ylabel('dropout')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: num_hops vs hidden_dim\n",
    "if len(df_valid) > 0 and 'num_hops' in df_valid.columns and 'hidden_dim' in df_valid.columns:\n",
    "    pivot_hops_dim = df_valid.pivot_table(\n",
    "        values=ERROR_COL, \n",
    "        index='hidden_dim', \n",
    "        columns='num_hops', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(pivot_hops_dim, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax)\n",
    "    ax.set_title(f'{ERROR_LABEL}: hidden_dim vs num_hops')\n",
    "    ax.set_xlabel('num_hops')\n",
    "    ax.set_ylabel('hidden_dim')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Performance by Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_valid) > 0:\n",
    "    print(\"Mean {} by hyperparameter:\\n\".format(ERROR_LABEL))\n",
    "    \n",
    "    for col in ['num_hops', 'num_layers', 'hidden_dim', 'dropout']:\n",
    "        if col in df_valid.columns:\n",
    "            means = df_valid.groupby(col)[ERROR_COL].agg(['mean', 'std', 'count'])\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(means.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_valid) > 0:\n",
    "    top10 = df_valid.nsmallest(10, ERROR_COL)\n",
    "    display_cols = ['num_hops', 'num_layers', 'hidden_dim', 'dropout', ERROR_COL, 'name']\n",
    "    available_display = [c for c in display_cols if c in top10.columns]\n",
    "    \n",
    "    print(\"Top 10 Configurations:\")\n",
    "    print(top10[available_display].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Based on the tuning results, update `configs/sparse-gt-rpearl-k5.yml` with the optimal hyperparameters and re-run the full e23 experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_valid) > 0:\n",
    "    best = df_valid.loc[df_valid[ERROR_COL].idxmin()]\n",
    "    \n",
    "    print(\"Recommended config update for sparse-gt-rpearl-k5.yml:\")\n",
    "    print()\n",
    "    print(\"sparse_gt:\")\n",
    "    if 'hidden_dim' in best.index:\n",
    "        print(f\"  hidden_dim: {int(best['hidden_dim'])}\")\n",
    "    if 'num_layers' in best.index:\n",
    "        print(f\"  num_layers: {int(best['num_layers'])}\")\n",
    "    print(f\"  num_heads: 4\")\n",
    "    if 'num_hops' in best.index:\n",
    "        print(f\"  num_hops: {int(best['num_hops'])}\")\n",
    "    print(f\"  rpearl_samples: 30\")\n",
    "    print(f\"  rpearl_num_layers: 5\")\n",
    "    if 'dropout' in best.index:\n",
    "        print(f\"  dropout: {best['dropout']}\")\n",
    "        print(f\"  attn_dropout: {best['dropout']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
